from keras import Model
from keras.layers import Input, Embedding, Bidirectional, GRU, Dense, TimeDistributed, Concatenate, Lambda, Add
from keras.layers import RepeatVector, Reshape, Dropout
import keras.backend as K
from keras.utils import np_utils
import numpy as np
from attention import Masked, MaskedTimeAttentionWithCoverage, MaskedGlobalMaxPooling1D, MaskedGlobalAveragePooling1D
from defined_layers import GetPiece, AttentionCopy, CombineGenCopy, MaskedSoftmax, ComputeAlpha, WeightedSum
from defined_layers import MaskedConv2D, MaskedAveragePooling2D, ComputeAttention, MaskedCopyProb


def CopyNetPlus(len_en, len_de, attr_num, embed_vocab_size, decode_vocab_size, m_embed_dim, w_embed_dim,
                hid_size, att_num, drop_rate, gen_mask, copy_mask):
    a = np.random.random([m_embed_dim])
    b = np.zeros([m_embed_dim])
    weight = np.array([[b, -a, b, a]])
    mark_embed_layer = Embedding(4, m_embed_dim, mask_zero=True, weights=weight, trainable=True)
    word_embed_layer = Embedding(embed_vocab_size, w_embed_dim, mask_zero=True)
    bi_rnn_layer1 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer2 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer3 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer4 = Bidirectional(GRU(hid_size, return_sequences=True))
    bi_rnn_layer5 = Bidirectional(GRU(hid_size, return_sequences=True))
    bi_rnn_layer6 = Bidirectional(GRU(hid_size))
    rnn_layer1 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    rnn_layer2 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    rnn_layer3 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    compute_alpha = ComputeAttention(att_num)
    p_gen_dense_layer = Dense(1, activation='sigmoid')
    gen_dense_layer = Dense(decode_vocab_size)
    dropout = Dropout(drop_rate)

    m_encoder_in = Input(shape=(len_en,), dtype=K.floatx())
    w_encoder_in = Input(shape=(len_en,), dtype=K.floatx())
    a_encoder_in = Input(shape=(len_en, attr_num), dtype=K.floatx())
    a_reshape_in = Lambda(lambda x: K.reshape(x, (-1, attr_num)))(a_encoder_in)
    m_embed_en = mark_embed_layer(m_encoder_in)
    w_embed_en = word_embed_layer(w_encoder_in)
    a_embed_en = word_embed_layer(a_reshape_in)
    embed_en = Concatenate()([m_embed_en, w_embed_en])
    rnn_h1, state_f1, state_b1 = bi_rnn_layer1(embed_en)
    rnn_h2, state_f2, state_b2 = bi_rnn_layer2(dropout(rnn_h1))
    rnn_h3, state_f3, state_b3 = bi_rnn_layer3(dropout(rnn_h2))
    a_rnn_h1 = bi_rnn_layer4(a_embed_en)
    a_rnn_h2 = bi_rnn_layer5(dropout(a_rnn_h1))
    a_rnn_h3 = bi_rnn_layer6(dropout(a_rnn_h2))
    a_rnn_h3 = Lambda(lambda x: K.reshape(x, (-1, len_en, hid_size * 2)))(dropout(a_rnn_h3))
    state1 = Concatenate()([state_f1, state_b1])
    state2 = Concatenate()([state_f2, state_b2])
    state3 = Concatenate()([state_f3, state_b3])
    rnn_h3 = dropout(rnn_h3)
    masked_rnn_h3, mask = Masked(return_mask=True)(rnn_h3)
    masked_rnn_h3 = Concatenate()([masked_rnn_h3, a_rnn_h3])
    encoder = Model(inputs=[m_encoder_in, w_encoder_in, a_encoder_in],
                    outputs=[masked_rnn_h3, mask, m_embed_en, state1, state2, state3])
    print(encoder.summary())

    token_in = Input(shape=(1,), dtype=K.floatx())
    state1_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    state2_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    state3_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    hid_state_en = Input(shape=(len_en, hid_size * 4), dtype=K.floatx())
    mask_en = Input(shape=(len_en,), dtype='bool')
    mark_en = Input(shape=(len_en, m_embed_dim))
    embed_de = word_embed_layer(token_in)
    rnn_h4_p, state_out1 = rnn_layer1(embed_de, initial_state=state1_p)
    rnn_h5_p, state_out2 = rnn_layer2(rnn_h4_p, initial_state=state2_p)
    rnn_h6_p, state_out3 = rnn_layer3(rnn_h5_p, initial_state=state3_p)
    alpha = compute_alpha([hid_state_en, rnn_h6_p, mask_en])
    att_cont = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, hid_state_en])
    att_mark = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, mark_en])
    p_gen_source = Concatenate()([rnn_h6_p, att_cont, embed_de])
    p_gen = p_gen_dense_layer(p_gen_source)
    att_out = Concatenate()([rnn_h6_p, att_cont, att_mark])
    gen_prob = TimeDistributed(gen_dense_layer)(att_out)
    gen_prob = MaskedSoftmax(gen_mask)(gen_prob)
    copy_prob = AttentionCopy(decode_vocab_size)([w_encoder_in, alpha])
    copy_prob = MaskedCopyProb(copy_mask)(copy_prob)
    next_token = CombineGenCopy()([p_gen, gen_prob, copy_prob])
    decoder = Model([token_in, w_encoder_in, hid_state_en, mask_en, mark_en, state1_p, state2_p, state3_p],
                    [next_token, p_gen, alpha, state_out1, state_out2, state_out3])
    print(decoder.summary())

    decoder_in = Input(shape=(len_de,), dtype=K.floatx())
    embed_de = word_embed_layer(decoder_in)
    rnn_h4, _ = rnn_layer1(embed_de, initial_state=state1)
    rnn_h5, _ = rnn_layer2(dropout(rnn_h4), initial_state=state2)
    rnn_h6, _ = rnn_layer3(dropout(rnn_h5), initial_state=state3)
    rnn_h6 = dropout(rnn_h6)
    alpha = compute_alpha([masked_rnn_h3, rnn_h6, mask])
    att_cont = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, masked_rnn_h3])
    att_mark = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, m_embed_en])
    att_cont = dropout(att_cont)
    att_mark = dropout(att_mark)
    p_gen_source = Concatenate()([rnn_h6, att_cont, embed_de])
    p_gen = p_gen_dense_layer(p_gen_source)
    att_out = Concatenate()([rnn_h6, att_cont, att_mark])
    gen_prob = TimeDistributed(gen_dense_layer)(att_out)
    gen_prob = MaskedSoftmax(gen_mask)(gen_prob)
    copy_prob = AttentionCopy(decode_vocab_size)([w_encoder_in, alpha])
    copy_prob = MaskedCopyProb(copy_mask)(copy_prob)
    output = CombineGenCopy()([p_gen, gen_prob, copy_prob])
    model = Model(inputs=[m_encoder_in, w_encoder_in, a_encoder_in, decoder_in], outputs=output)
    print(model.summary())
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    return model, encoder, decoder


def CopyNetPlusNoCopy(len_en, len_de, attr_num, embed_vocab_size, decode_vocab_size, m_embed_dim, w_embed_dim,
                      hid_size, att_num, drop_rate):
    a = np.random.random([m_embed_dim])
    b = np.zeros([m_embed_dim])
    weight = np.array([[b, -a, b, a]])
    mark_embed_layer = Embedding(4, m_embed_dim, mask_zero=True, weights=weight, trainable=True)
    word_embed_layer = Embedding(embed_vocab_size, w_embed_dim, mask_zero=True)
    bi_rnn_layer1 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer2 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer3 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer4 = Bidirectional(GRU(hid_size, return_sequences=True))
    bi_rnn_layer5 = Bidirectional(GRU(hid_size, return_sequences=True))
    bi_rnn_layer6 = Bidirectional(GRU(hid_size))
    rnn_layer1 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    rnn_layer2 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    rnn_layer3 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    compute_alpha = ComputeAttention(att_num)
    gen_dense_layer = Dense(decode_vocab_size, activation='softmax')
    dropout = Dropout(drop_rate)

    m_encoder_in = Input(shape=(len_en,), dtype=K.floatx())
    w_encoder_in = Input(shape=(len_en,), dtype=K.floatx())
    a_encoder_in = Input(shape=(len_en, attr_num), dtype=K.floatx())
    a_reshape_in = Lambda(lambda x: K.reshape(x, (-1, attr_num)))(a_encoder_in)
    m_embed_en = mark_embed_layer(m_encoder_in)
    w_embed_en = word_embed_layer(w_encoder_in)
    a_embed_en = word_embed_layer(a_reshape_in)
    embed_en = Concatenate()([m_embed_en, w_embed_en])
    rnn_h1, state_f1, state_b1 = bi_rnn_layer1(embed_en)
    rnn_h2, state_f2, state_b2 = bi_rnn_layer2(dropout(rnn_h1))
    rnn_h3, state_f3, state_b3 = bi_rnn_layer3(dropout(rnn_h2))
    a_rnn_h1 = bi_rnn_layer4(a_embed_en)
    a_rnn_h2 = bi_rnn_layer5(dropout(a_rnn_h1))
    a_rnn_h3 = bi_rnn_layer6(dropout(a_rnn_h2))
    a_rnn_h3 = Lambda(lambda x: K.reshape(x, (-1, len_en, hid_size * 2)))(dropout(a_rnn_h3))
    state1 = Concatenate()([state_f1, state_b1])
    state2 = Concatenate()([state_f2, state_b2])
    state3 = Concatenate()([state_f3, state_b3])
    rnn_h3 = dropout(rnn_h3)
    masked_rnn_h3, mask = Masked(return_mask=True)(rnn_h3)
    masked_rnn_h3 = Concatenate()([masked_rnn_h3, a_rnn_h3])
    encoder = Model(inputs=[m_encoder_in, w_encoder_in, a_encoder_in],
                    outputs=[masked_rnn_h3, mask, m_embed_en, state1, state2, state3])
    print(encoder.summary())

    token_in = Input(shape=(1,), dtype=K.floatx())
    state1_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    state2_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    state3_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    hid_state_en = Input(shape=(len_en, hid_size * 4), dtype=K.floatx())
    mask_en = Input(shape=(len_en,), dtype='bool')
    mark_en = Input(shape=(len_en, m_embed_dim))
    embed_de = word_embed_layer(token_in)
    rnn_h4_p, state_out1 = rnn_layer1(embed_de, initial_state=state1_p)
    rnn_h5_p, state_out2 = rnn_layer2(rnn_h4_p, initial_state=state2_p)
    rnn_h6_p, state_out3 = rnn_layer3(rnn_h5_p, initial_state=state3_p)
    alpha = compute_alpha([hid_state_en, rnn_h6_p, mask_en])
    att_cont = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, hid_state_en])
    att_mark = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, mark_en])
    att_out = Concatenate()([rnn_h6_p, att_cont, att_mark])
    next_token = TimeDistributed(gen_dense_layer)(att_out)
    decoder = Model([token_in, w_encoder_in, hid_state_en, mask_en, mark_en, state1_p, state2_p, state3_p],
                    [next_token, alpha, state_out1, state_out2, state_out3])
    print(decoder.summary())

    decoder_in = Input(shape=(len_de,), dtype=K.floatx())
    embed_de = word_embed_layer(decoder_in)
    rnn_h4, _ = rnn_layer1(embed_de, initial_state=state1)
    rnn_h5, _ = rnn_layer2(dropout(rnn_h4), initial_state=state2)
    rnn_h6, _ = rnn_layer3(dropout(rnn_h5), initial_state=state3)
    rnn_h6 = dropout(rnn_h6)
    alpha = compute_alpha([masked_rnn_h3, rnn_h6, mask])
    att_cont = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, masked_rnn_h3])
    att_mark = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha, m_embed_en])
    att_cont = dropout(att_cont)
    att_mark = dropout(att_mark)
    att_out = Concatenate()([rnn_h6, att_cont, att_mark])
    output = TimeDistributed(gen_dense_layer)(att_out)
    model = Model(inputs=[m_encoder_in, w_encoder_in, a_encoder_in, decoder_in], outputs=output)
    print(model.summary())
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    return model, encoder, decoder


def CopyNetPlus2(len_en, len_de, embed_vocab_size, decode_vocab_size, m_embed_dim, w_embed_dim,
                hid_size, att_num, drop_rate, gen_mask, copy_mask):
    a = np.random.random([m_embed_dim])
    b = np.zeros([m_embed_dim])
    weight = np.array([[b, -a, b, a]])
    mark_embed_layer = Embedding(4, m_embed_dim, mask_zero=True, weights=weight, trainable=True)
    word_embed_layer = Embedding(embed_vocab_size, w_embed_dim, mask_zero=True)
    bi_rnn_layer1 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer2 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer3 = Bidirectional(GRU(hid_size, return_sequences=True, return_state=True))
    bi_rnn_layer4 = Bidirectional(GRU(hid_size, return_sequences=True))
    bi_rnn_layer5 = Bidirectional(GRU(hid_size, return_sequences=True))
    bi_rnn_layer6 = Bidirectional(GRU(hid_size, return_sequences=True))
    rnn_layer1 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    rnn_layer2 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    rnn_layer3 = GRU(hid_size * 2, return_sequences=True, return_state=True)
    compute_alpha = ComputeAttention(att_num)
    p_gen_dense_layer = Dense(1, activation='sigmoid')
    gen_dense_layer = Dense(decode_vocab_size)
    dropout = Dropout(drop_rate)

    m_encoder_in = Input(shape=(len_en,), dtype=K.floatx())
    w_encoder_in = Input(shape=(len_en,), dtype=K.floatx())
    a_encoder_in = Input(shape=(len_en,), dtype=K.floatx())
    m_embed_en = mark_embed_layer(m_encoder_in)
    w_embed_en = word_embed_layer(w_encoder_in)
    a_embed_en = word_embed_layer(a_encoder_in)
    embed_en = Concatenate()([m_embed_en, w_embed_en])
    rnn_h1, state_f1, state_b1 = bi_rnn_layer1(embed_en)
    rnn_h2, state_f2, state_b2 = bi_rnn_layer2(dropout(rnn_h1))
    rnn_h3, state_f3, state_b3 = bi_rnn_layer3(dropout(rnn_h2))
    a_rnn_h1 = bi_rnn_layer4(a_embed_en)
    a_rnn_h2 = bi_rnn_layer5(dropout(a_rnn_h1))
    a_rnn_h3 = bi_rnn_layer6(dropout(a_rnn_h2))
    a_rnn_h3 = dropout(a_rnn_h3)
    state1 = Concatenate()([state_f1, state_b1])
    state2 = Concatenate()([state_f2, state_b2])
    state3 = Concatenate()([state_f3, state_b3])
    rnn_h3 = dropout(rnn_h3)
    masked_rnn_h3, mask = Masked(return_mask=True)(rnn_h3)
    m_a_rnn_h3, m_a = Masked(return_mask=True)(a_rnn_h3)
    encoder = Model(inputs=[m_encoder_in, w_encoder_in, a_encoder_in],
                    outputs=[masked_rnn_h3, mask, m_a_rnn_h3, m_a, m_embed_en, state1, state2, state3])
    print(encoder.summary())

    token_in = Input(shape=(1,), dtype=K.floatx())  # current token
    state1_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    state2_p = Input(shape=(hid_size * 2,), dtype=K.floatx())
    state3_p = Input(shape=(hid_size * 2,), dtype=K.floatx())   # state input
    hid_state_en = Input(shape=(len_en, hid_size * 2), dtype=K.floatx())
    mask_en = Input(shape=(len_en,), dtype='bool')  # function information input and it's mask
    at_state_en = Input(shape=(len_en, hid_size * 2), dtype=K.floatx())
    mask_at = Input(shape=(len_en,), dtype='bool')  # textual information input and it's mask
    mark_en = Input(shape=(len_en, m_embed_dim))    # mark information for final generate
    embed_de = word_embed_layer(token_in)
    rnn_h4_p, state_out1 = rnn_layer1(embed_de, initial_state=state1_p)
    rnn_h5_p, state_out2 = rnn_layer2(rnn_h4_p, initial_state=state2_p)
    rnn_h6_p, state_out3 = rnn_layer3(rnn_h5_p, initial_state=state3_p)
    alpha1 = compute_alpha([hid_state_en, rnn_h6_p, mask_en])
    att_cont = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha1, hid_state_en])
    att_mark = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha1, mark_en])
    alpha2 = compute_alpha([at_state_en, rnn_h6_p, mask_at])
    att_at = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha2, at_state_en])
    p_gen_source = Concatenate()([rnn_h6_p, att_cont, att_at, embed_de])
    p_gen = p_gen_dense_layer(p_gen_source)
    att_out = Concatenate()([rnn_h6_p, att_cont, att_mark, att_at])
    gen_prob = TimeDistributed(gen_dense_layer)(att_out)
    gen_prob = MaskedSoftmax(gen_mask)(gen_prob)
    copy_prob = AttentionCopy(decode_vocab_size)([w_encoder_in, alpha1])
    copy_prob = MaskedCopyProb(copy_mask)(copy_prob)
    next_token = CombineGenCopy()([p_gen, gen_prob, copy_prob])
    decoder = Model([token_in, w_encoder_in, hid_state_en, mask_en, at_state_en, mask_at,
                     mark_en, state1_p, state2_p, state3_p],
                    [next_token, p_gen, alpha1, alpha2, state_out1, state_out2, state_out3])
    print(decoder.summary())

    decoder_in = Input(shape=(len_de,), dtype=K.floatx())
    embed_de = word_embed_layer(decoder_in)
    rnn_h4, _ = rnn_layer1(embed_de, initial_state=state1)
    rnn_h5, _ = rnn_layer2(dropout(rnn_h4), initial_state=state2)
    rnn_h6, _ = rnn_layer3(dropout(rnn_h5), initial_state=state3)
    rnn_h6 = dropout(rnn_h6)
    alpha1 = compute_alpha([rnn_h3, rnn_h6])
    alpha2 = compute_alpha([a_rnn_h3, rnn_h6])
    att_cont = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha1, rnn_h3])
    att_mark = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha1, m_embed_en])
    att_at = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(2, 1)))([alpha2, a_rnn_h3])
    att_cont = dropout(att_cont)
    att_mark = dropout(att_mark)
    att_at = dropout(att_at)
    p_gen_source = Concatenate()([rnn_h6, att_cont, att_at, embed_de])
    p_gen = p_gen_dense_layer(p_gen_source)
    att_out = Concatenate()([rnn_h6, att_cont, att_mark, att_at])
    gen_prob = TimeDistributed(gen_dense_layer)(att_out)
    gen_prob = MaskedSoftmax(gen_mask)(gen_prob)
    copy_prob = AttentionCopy(decode_vocab_size)([w_encoder_in, alpha1])
    copy_prob = MaskedCopyProb(copy_mask)(copy_prob)
    output = CombineGenCopy()([p_gen, gen_prob, copy_prob])
    model = Model(inputs=[m_encoder_in, w_encoder_in, a_encoder_in, decoder_in], outputs=output)
    print(model.summary())
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    return model, encoder, decoder


if __name__ == '__main__':
    model, encoder, decoder = CopyNetPlus(100, 20, 5, 10000, 64, 128, 128, 64, 0.1, np.ones((10000,)), np.ones((10000,)))
    model, encoder, decoder = CopyNetPlus2(100, 20, 10000, 64, 128, 128, 64, 0.1, np.ones((10000,)),
                                           np.ones((10000,)))
